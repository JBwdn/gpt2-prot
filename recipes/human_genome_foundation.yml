# lightning.pytorch==2.3.0
seed_everything: 0
ckpt_path: last  # Loads the most recent checkpoint in `checkpoints/`

trainer:
  max_epochs: 1000
  log_every_n_steps: 25
  fast_dev_run: true
  enable_checkpointing: true
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        save_dir: "."
  callbacks: 
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "checkpoints/"  # Needs to be set for ckpt_path to correctly load `last`
        save_last: true
    - class_path: PreviewCallback
      init_args:
        mode: "nt"
        prompt: "ATG"
        length: 75

model:
  config:
    vocab_size: 5
    window_size: 1024
    n_layers: 8
    n_heads: 8
    embed_d: 1024
    emb_dropout: 0.1
    attn_dropout: 0.1
    res_dropout: 0.1
    adam_lr: 0.0003
    adam_weight_decay: 0.1
    adam_betas: [0.90, 0.95]

data:
  mode: "nt"
  directory: "seqs/"
  batch_size: 1 
  max_seq_length: 10000
  n_seq_limit: 1000
  loader_num_workers: 2
  downloads: [
    ["https://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr1.fa.gz", "chr1.fa.gz"],
    ["https://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr2.fa.gz", "chr2.fa.gz"]
    # etc...
  ]
